<article>
  <type>article</type>
  <title>Sensor Technology Opens New Horizons</title>
  <author>Henry S. Kenyon</author>
  <date>June 2008</date>
  <departments>
  </departments>
  <tags>
    <tag>June 2008</tag>
    <tag>Next-Generation Technologies</tag>
  </tags>
  <abstract>In the future, there will be no place to hide from the U.S. military. Two prototype sensor technologies may soon allow warfighters to observe enemy units at great distances and to track their movement inside buildings and urban areas. These systems benefit from recent developments in optics, radar, algorithms and processing to pull images out of desert heat distortion or to create maps of entire neighborhoods rapidly, greatly increasing soldiers’ situational awareness.</abstract>
  <text> DARPA’s Visibuilding program will enable warfighters to map buildings prior to urban operations. The radar-based technology also can detect the presence of large quantities of metal, such as a weapons cache, and it can track the movements of individual people in a building. Innovative optics, radar systems cut through the fog of battle.In the future, there will be no place to hide from the U.S. military. Two prototype sensor technologies may soon allow warfighters to observe enemy units at great distances and to track their movement inside buildings and urban areas. These systems benefit from recent developments in optics, radar, algorithms and processing to pull images out of desert heat distortion or to create maps of entire neighborhoods rapidly, greatly increasing soldiers’ situational awareness.These systems are part of an effort by the Defense Advanced Research Projects Agency (DARPA) to increase warfighters’ operational capabilities while decreasing an adversary’s ability to surprise or outmaneuver U.S. forces. The Super Resolution Vision System (SRVS) program will develop a sniper/reconnaissance telescope capable of observing targets at longer distances than currently possible with unaided optics, while the Visibuilding program will design a radar-based surveillance system to map buildings and track movement in those structures.  Atmospheric turbulence in the form of heat shimmer can greatly limit the distance ground-based electro-optical and infrared systems can observe and track terrestrial objects. The SRVS relies on an atmospheric quirk and raw signal processing power to stitch a complete image out of distorted fragments. The technology exploits a phenomenon known as turbulence-generated micro-lensing, which creates a brief, high-resolution “lucky” image out of a distorted scene. Scientists have been aware of this effect since the 1970s, explains SRVS program manager Dr. Jennifer C. Ricklin. If an object is scanned long enough with a high-speed camera, lucky images will appear. However, she notes, it soon became evident that it was impractical to scan for such images because the statistical probability of reliably capturing a lucky image was very low. In the late 1990s it was demonstrated that although a complete image could not be regularly captured with this technique, it was possible to retrieve a piece of it. A high-speed camera could theoretically pick out the good parts of a distorted image and assemble them into a coherent, high-resolution picture. She adds that SRVS is now possible because of increased processing power and advances in signal processing algorithms. “It’s really an enormous signal processing issue,” she says. If a warfighter were using an SRVS-based system to observe a cluster of buildings and vehicles, the optics would pull an image together from scattered light that may not even appear in the scope’s center of focus. Ricklin explains that because of the refractive properties in the atmosphere, light does not travel in a straight path but zigzags every time it encounters small areas of differing atmospheric pressure and refractivity. Atmospheric turbulence, most commonly visible as heat shimmer, is most active in the first few meters above the ground and decreases exponentially with altitude. “You’re taking advantage of the fact that some rays that would not be captured by the [telescope’s] optical diameter are accidentally refracted into the focal plane from the outside. It happens very randomly and only for parts of the image, not the entire image. But if you have the algorithms and the hardware, you can take advantage of this,” she says.The program recently passed its important first phase, which demonstrated the basic practicality of the technology. With the go-no-go tests complete, the next phase will focus on refining the technology. The final goal of the program is to develop a reconnaissance-spotter scope in the same form factor as the existing scope used by U.S. forces. Ricklin notes that this is a twofold problem. The first challenge is to demonstrate that this physical phenomenon can be exploited, which had never been done before outside of a laboratory. The second goal is to develop a system and fit it into a form factor with the necessary processing and service requirements for use by warfighters. “Nobody wants to carry an optical bench on their backs. It needs to be quite ruggedized,” she says.During phase one of the program, DARPA scientists conducted tests at Sandia National Laboratories in New Mexico to determine how far U.S. Army and Marine Corps snipers could see with conventional optics. Ricklin notes that no one had ever actually asked snipers how far they could see a target through atmospheric turbulence. Although the marksmen said they could pick out objects at hundreds of meters, she shares that their claims had never been scientifically measured. The Sandia tests measured the differences between current capabilities and SRVS-aided viewing. Although she cannot provide any specific details, she said that the capability greatly increased the distance over which snipers could accurately detect targets. She adds that the SRVS’s optical technology can be applied to any type of system that is affected by atmospheric turbulence, such as long-range surveillance systems and weapons sights.If tracking targets at extreme visual range is challenging for U.S. forces, an even more pressing concern is locating enemy units hiding in buildings. The Visibuilding program is designed to meet the military’s need to extend intelligence, surveillance and reconnaissance capabilities in urban operations. Visibuilding relies on radio frequency (RF) technology, predominantly radar, to scan structures. The main challenge is developing the means to gain deeper RF penetration into buildings and forming the reflected signals into an accurate building model, says Program Manager Dr. Edward J. Baranoski. He explains that DARPA’s approach is very different from traditional radar techniques, such as synthetic aperture radar (SAR) systems. Conventional radars can image relatively clearly in open space, but in high, multipath environments, such as in built-up urban areas, sensing becomes more difficult. DARPA scientists are trying to improve sensing characteristics to sharpen and produce more data than a standard SAR image, which only produces data when it scans a building’s internal spaces. “We are trying to give warfighters the layout of the building—where insurgents would be inside the building and also to help find large quantities of materials that are inconsistent with the building. This particular technology won’t be able to identify materials, but if you have two tons of metal in an upstairs bedroom, it could be a weapons cache—that certainly is not consistent with the standard use of a residential building,” Baranoski shares. He notes that it is not very difficult to transmit RF energy into a building, but the challenge is in making sense of the data produced from all the reflected signals. Besides processing data from the inside a structure, the system also must filter a large amount of RF propagation in the form of randomly reflected signals. Although radar technologies exist that can track people in adjacent rooms, it is much more difficult to map an entire building. “Going through one wall is not that bad, but a building is basically an RF hall of mirrors. You’ve got signals bouncing all over the place,” he says.The program is examining a range of sensor suites with different capabilities. These systems would be mounted on airborne and ground vehicles or emplaced sensors. For example, a one- or two-story building can be scanned effectively in a drive-by sweep with a truck-mounted sensor. But a 10-story building would require sensors on rooftops or nearby buildings to scan the upper floors. An airborne system would provide surveillance for a large area such as a neighborhood by generating a first-pass map and highlighting anomalies such as the locations of weapons caches. As U.S. forces approach, different systems would provide more detailed images, some hours ahead of time for selected buildings. Visibuilding radar data can be displayed on a variety of systems, such as maps on computer displays or personal digital assistants. The displays could include fly-throughs and maps with moving dots to indicate the location of people in a building. “Your processing time is on the order of how far in advance you’re actually doing the sensing to get these things,” Baranoski explains. Data from an airborne survey may take a day to process, and a more detailed building surveillance could take place an hour before an operation, while tracking individual people moving in a building would happen in real time. He adds that in each stage, data processing moves closer to real time to provide situational awareness consistent with the operational time frame. Visibuilding is entering its second phase. Baranoski notes that the first phase demonstrated the technology’s proof of concept and that its algorithms worked on highly detailed RF simulations. He adds that in the first phase the technology was able to produce highly accurate layouts of buildings, as opposed to SAR radar smears. Phase two consists of building prototype systems and trying out the technology on existing structures. The second phase will test the entire system, and phase three will develop a rugged operational technology. Another feature in the third phase will be the requirement that individual sensors must wirelessly communicate and share imaging data with each other. Phase two is 18 months long and ends in July 2009. Field trials are scheduled for fall 2008 and spring 2009.Web ResourceDefense Advanced Research Projects Agency: www.darpa.mil </text>
  <imgalttext>
  </imgalttext>
</article>
