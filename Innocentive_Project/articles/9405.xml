<article>
  <type>blog</type>
  <title>Health Care 'Big Data' Is a Big Opportunity to Address Data Overload</title>
  <author>Roger Foster, SIGNAL Scape Guest Blogger</author>
  <date>April 11, 2012</date>
  <departments>
  </departments>
  <tags>
    <tag>DRC</tag>
    <tag>guest bloggers</tag>
    <tag>Health IT</tag>
    <tag>Guest Blogs</tag>
  </tags>
  <abstract>We are drowning in health care data, but remain thirsty for information. Driven by federal incentives, public and private healthcare institutions have generated billions of electronic health records (EHRs), rich with clinical and demographic data. This data can be aggregated and analyzed to improve scientific research, drug safety and overall public health. However, the growth in health care data is so fast that issues surrounding data storage, integrity, modeling and sharing can diminish its tremendous potential. Guest blogger Roger Foster of DRC looks at this important health IT issue.</abstract>
  <text>"Water, water everywhere and not a drop to drink."
-Rime of the Ancient Mariner by Samuel Coleridge
We are drowning in health care data, but remain thirsty for information. Driven by federal incentives, public and private health care institutions have generated billions of electronic health records (EHRs), rich with clinical and demographic data. This data can be aggregated and analyzed to improve scientific research, drug safety and overall public health. A McKinsey Global Health Institute study projected that if U.S. health care used Big Data creatively and effectively to drive efficiency and quality, the sector could create more than $300 billion in value every year. However, the growth in health care data is so fast that issues surrounding data storage, integrity, modeling and sharing can diminish its tremendous potential. Graham Huges estimates the growth in health care data to be between 1.2 to 2.4 exabytes per year.  This number represents roughly 10 times the data contained in every U.S. academic research library combined. This data is also largely disparate and unstructured making cross-study and extracting useful information all the more problematic. Processing Big Data requires sifting through huge data sets to identify salient information and then using that information to make informed decisions. Just like Amazon can use your book buying history coupled with the population information of all book buyers to make selected book recommendations; health care providers can use information in your EHR to make risk assessment of various medical outcomes based on population data. This approach is changing how we manage clinical, operational and financial data across the health care system. Scientists, who have been able to work with independent data sets, must now figure out how to coordinate multiple data sets to improve outcomes. It is not always an easy transition. But it is a transition that government health agencies have wisely already begun in order to improve public health and better fulfill their mission requirements. For example, the Centers for Disease Control and Prevention (CDC) is capturing health record data in near real time as part of its bio-terrorism early detection program. The Food and Drug Administration (FDA) is currently developing Janus and Sentinel, repositories to study pre-market assessment and post-market surveillance of drugs and medical devices. As these first adopters move toward fully utilizing Big Data, special attention and adequate resources should be dedicated to architecture.  Systems and data architecture are two of the keys that will unlock the potential of Big Data. For example, National Oceanic and Atmospheric Association (NOAA) scientists modeling climate change at the Geophysical Fluid Dynamics Laboratory in Princeton are supported by data architects. The architects' job is to ensure the petabytes of climate model data generated by NOAA scientists are securely transferred and stored in accordance with the requirements for accuracy demanded by the scientific community, and building analytical tools to interpret and publish this data for use by other scientists. System architects have been steadily upgrading the high performance computing hardware over a period of four years. As a result, the time it takes for scientists to run their simulations and studies has been cut in half. Data sharing and exchange play a critical part in making the most out of analytics investments. Most analytic capabilities in health care are not as advanced compared to those in other fields such as weather modeling, or even finance, retail and manufacturing. Government health agencies are moving in the right direction. With shrewd investments, architecture and foresight, the albatross of health and electronic health record data overload can be transformed into a valuable resource that helps improve public health. Roger Foster is a Senior Director at DRC's High Performance Technologies Group and board member of the Technology Management program at George Mason University. He has over 20 years of leadership experience in strategy, technology management and operations support for government agencies and commercial businesses.  He has worked big data problems for scientific computing in fields ranging from large astrophysical data sets to health information technology. He has a master's degree in Management of Technology from the Massachusetts Institute of Technology and a doctorate in Astronomy from the University of California, Berkeley. He can be reached at rfoster@drc.com. The views expressed by our guest bloggers are their own and do not necessarily reflect the views of AFCEA International or SIGNAL Magazine. More AFCEA and SIGNAL resources on Health IT: Health IT Directory Two Government Organizations, One Health Information System Frontline Care, Lifetime Benefits Mine-Hunting Technology Learns to Fight Cancer</text>
  <imgalttext>
  </imgalttext>
</article>
