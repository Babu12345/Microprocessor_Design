<article>
  <type>article</type>
  <title>A New Chip Thinks Like a Brain</title>
  <author>Max Cacas</author>
  <date>March 1, 2013</date>
  <departments>
    <department>Cyber</department>
  </departments>
  <tags>
    <tag>Army Technologies</tag>
    <tag>data management</tag>
    <tag>research and development</tag>
  </tags>
  <abstract></abstract>
  <text>An Army research team develops a device that could assist warfighters' decision making.
A U.S. Army scientist and his colleagues, working in the nascent field of neural computing and quantum physics, have earned a patent for a powerful quantum neural dynamics computer chip. The device, which has been tested in a laboratory, and the advanced mathematical computations that make it work may lead one day to powerful devices that could help warfighters sift through huge datasets of information and make important tactical decisions in the field. The chip also holds promise for civilian applications requiring the rapid analysis of big data, and it could represent a bridge to the next generation of computing.
“The patent covers different ways to make computer chips,” states Ron Meyers, a computer scientist with the Army Research Laboratory (ARL) who is the principal investigator for the neural chip project. “We developed a type of mathematics that allows for quick function-changing and also emulates some of the processes of neural intelligence that the human brain uses. We combined those together, and we made a new type of computer chip that incorporates those functions. It’s qualitatively different. It doesn’t do the same kinds of computations as traditional computer chips.”
	The chip, and its underlying operating system based on newly developed mathematical formulas, will make possible faster and more powerful computers. “We’re talking about the ability to compute that exceeds exponentially millions of times greater than any of the computers that exist today or are on the drawing boards using conventional approaches,” Meyers explains.
	Whether it is government intelligence analysts sifting through mountains of data from satellites and unmanned aerial vehicles or corporate marketing experts mining data to find the next hotly sought-after gadget, they need new tools and techniques to help deal with the challenges presented by big data, scientists say. Along with the limits of traditional analytical software, one of the biggest shortcomings outlined by computing experts is that of processor speed and the related inability to process quickly datasets that extend into the petabyte range.
	Because the focus of the ARL is on the needs of the warfighter, Meyers’ first priority is in looking at how the device he has helped develop can provide advantages to the military. Meyers envisions troops equipped with high-speed quantum computing assistants networked together that constantly can communicate and exchange the latest information.
	The chip could become the basis for a computing device, possibly small enough to be held in one’s hand, which could help a soldier deal with large volumes of data from sources on the Internet, from sensors on equipment that he and other soldiers carry and other sources, Meyers offers. Such a device could go a long way toward minimizing unintended consequences, including mistakes in directing air-to-ground tactical strikes using drones, or underestimating the size of opposing forces. “It could also help determine the best course of action, putting all their intelligence together, and help them come to the optimal solution,” Meyers says.
	Explaining another possible application, Meyers says the sheer speed and capability of the new chip are ideally suited to the task of determining what kind of information is genuine and what might be false. “The chip could potentially be used to sift out motives or types of operations directed against the user or his systems,” he explains. In the realm of cybersecurity, the chip’s speed could be useful in developing the next generation of user authentication or data encryption.
	One key to understanding the speed and data processing power of the neural chip is the fact that it emulates the way that neurons in the human brain process information. “It helps get information; discern it; separate it from random signals and determine useful patterns,” Meyers explains. The result is a computer chip that is qualitatively different from those found in traditional devices ranging from the latest smartphones to supercomputers.
	“The way it’s designed, it has neurons; it has synaptic connections between the different neurons; it has memory; and so it has a lot of the aspects that a biological system such as the brain has. You’re really talking about how to program those in a very streamlined fashion,” he says.
	The other important key to understanding the neural chip is that it employs what Meyers calls a new type of fast mathematics. Several pages of the patent now on file with the U.S. Patent and Trademark Office (USPTO) are devoted to explaining in highly technical detail the non-Lipschitz mathematics that are the underlying basis for how the chip works. In layman’s terms, the chip operates employing highly advanced non-standard mathematics using functions to define and make possible rapid change. “It [the chip] can take direct paths toward solutions and doesn’t have to go through the usual operations,” he says. This means that by using non-traditional mathematics, Meyers and his colleagues have functionally removed the limit on how fast a computational function can change. That, he says, is one more step on the road to ultra high-speed computing. By removing that limit, the chip also is able to emulate the brain’s ability to adapt synaptic functions in neurons and to learn through feedback.
	To facilitate the operation of functional devices using the chip, Meyers says, some current software applications could be adapted for use, but others might have to be programmed from scratch. “What you’re building into it is a new type of hardware logic. Normal computers deal with ones and zeroes, and they are digital. This computer based on the neural chip can use analog signals, and yet it can also be controlled by programming it with some of the standard languages. Eventually, you do want a different language to take advantage of some of the operations that it can do that the existing class of computers wouldn’t perform very well,” he explains.
	The neural chip also can be considered a bridge to the holy grail of next-generation computing, the quantum computer. That device, which is still the province of the most advanced scientists and mathematicians, would harness individual atoms to both store and process data. The fact that the neural chip emulates biological systems will play a key role in developing the quantum computer, Meyers emphasizes. “What’s different about it is the type of mathematics. It’s a non-standard mathematics that it [the quantum computer] implements,” he explains, which is similar to the fast mathematics employed by the neural chip. The key is making the mathematics scalable and able to cope with processing exponentially large numbers of computing instructions and datasets. For his part, Meyers is using his neural chip research at the ARL for another project that will help develop networks and communications systems to harness quantum computing.
	The neural chip is the result of several years of work on the part of Meyers and his colleagues, who include Army mathematician Keith Deacon and Dr. Gert Cauwenberghs, a professor of bioengineering and biology who also is co-director of the Institute for Neural Computation at the University of California at San Diego. Cauwenberghs helped adapt the mathematics developed by Meyers and Deacon into what the patent describes as an analog “continuous-time neural architecture.” Cauwenberghs then translated that architecture into a working integrated circuit that served as the proof of concept that the neural chip concept worked. That led to the USPTO issuing a patent for the device on September 11, 2012.
	Meyers says his work on the neural chip builds upon decades of research conducted by other mathematicians and computer scientists doing similar research.
	 
 </text>
  <imgalttext>
  </imgalttext>
</article>
